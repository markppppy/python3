[toc]

# 统计学习方法(机器学习)

## 特征选择
为什么要进行特征选择：
1、删除对模型分类没有帮助的特征，避免过拟合、减少运算时间；
2、相关

1、选择角度：缺失值、相关性、重要性、特征值都是否相同；可用的包：feature_selector

2、主成分分析主要用来降维

## 极大似然估计和贝叶斯估计
问题: 用贝叶斯估计数据集D ~ N(μ, $\sigma^2$)的过程中，P(μ)为什么不是固定值

## 监督学习

### 单分类器

#### 支持向量机


#### 决策树


#### 逻辑回归
1. 分布函数、密度函数、函数对应的曲线

2. 和线性分布的异同

3. 如何使用LR处理多标签分类；多标签分类，分为一个样本有多个标签和一个样本一个标签的情况；


#### 神经网络

- 感知机

#### 最大熵

思想：已知事件的概率按已知的来，未知事件的概率默认均匀分布；

相关：特征函数(采集的样本中有噪音数据，特征函数可以去除噪音？)，经验分布，实际分布

#### 隐马尔可夫模型
状态链、观测链
假设: 齐次马尔可夫性; 观测独立假设

### 多分类器(集成学习)

#### GBDT
- XGBoost

<br>

- LGBM: 一个实现GBDT算法的框架，支持高效率的并行训练
  - 优点: 有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据


#### 随机森林(bagging)

- 简述：生成多棵决策树，以bagging的方式决定最后的结果；
  <br>

- 每个决策树生成的规则：
  1. 如果训练集的样本数为N, 每棵树的训练集就是从N中随机有放回的按次取N个样本，作为每个树的训练集；(即：每棵树的训练集样本数相同但样本不同，并且样本会重复)
  2. 如果样本特征维度为M, 每棵树随机从中选取m个特征(m < M)，进行生长；
  3. 每棵树都以最大程度生长，不剪枝；
  - 特点总结：上述过程经过了两个随机，一是样本随机，二是选取特征范围随机；这使得随机森林不容易过拟合，并且有对样本数据不敏感的特点；
    <br>

- 影响模型效果的因素
  - 森林中任意两棵树的相关性，相关性越大，错误率越高；
  - 森林中每棵树的分类能力，分类能力越好，整体模型效果越好；

- 调优
  - 每棵树特征选择的个数m，m越大，任意两棵树的相关性越大，单棵树的分类能力越强；
  - 如果减小m，增大树深，相关性减小，单树分类能力增加，是不是效果更好？可能单树的分类能力不如增大m的时候？

- 验证模型效果的方法
  - 袋外错误率(out-of-bag error)；随机森林因为每棵树的生成都没有用到全部样本，所以每棵树可以用剩下的数据作为测试数据；


### 标注问题

## 半监督学习

## 无监督学习

## 强化学习

## 元学习(监督学习？)

思想：通过多个分类器样本的参数的映射，学习到规律；
应用：在一个新的训练任务中，模型初始化参数可以使用之前学到的规律，根据样本预测，而不用随机初始化。

# 深度学习相关

- 优点：端到端，没有过多人为处理(少出错)
- 特点：表征学习，通过端到端的训练，发现更好的特征，而后面用于分类（或其他任务）的函数，往往只是普通的

## DeepFM(FM结合DNN系列)

数值类型的特征怎么加入深度模型如nfm，deepfm: https://www.zhihu.com/question/352399723

### FM、SVM

FM和SVM的区别、FM、FFM、DeepFM：https://blog.csdn.net/hiwallace/article/details/81333604

## NFM(FM结合DNN系列)


# 参考文档
lgbm
https://github.com/jiangnanboy/learning_to_rank
https://zhuanlan.zhihu.com/p/99069186

SHAP值解释模型特征
https://blog.csdn.net/u011984148/article/details/105803720

无偏估计：不考虑样本的出现概率，认为概率相同；

随机森林
https://blog.csdn.net/yangyin007/article/details/82385967
还有效果图

DeepFM
https://www.jianshu.com/p/152ae633fb00

相关面试题
https://www.jianshu.com/nb/14191364


