[toc]

# 树模型和相关理论

## 随机森林

- 简述：生成多棵决策树，以bagging的方式决定最后的结果；
<br>

- 每个决策树生成的规则：
  1. 如果训练集的样本数为N, 每棵树的训练集就是从N中随机有放回的按次取N个样本，作为每个树的训练集；(即：每棵树的训练集样本数相同但样本不同，并且样本会重复)
  2. 如果样本特征维度为M, 每棵树随机从中选取m个特征(m < M)，进行生长；
  3. 每棵树都以最大程度生长，不剪枝；
  - 特点总结：上述过程经过了两个随机，一是样本随机，二是选取特征范围随机；这使得随机森林不容易过拟合，并且有对样本数据不敏感的特点；
<br>

- 影响模型效果的因素
  - 森林中任意两棵树的相关性，相关性越大，错误率越高；
  - 森林中每棵树的分类能力，分类能力越好，整体模型效果越好；

- 调优
  - 每棵树特征选择的个数m，m越大，任意两棵树的相关性越大，单棵树的分类能力越强；
  - 如果减小m，增大树深，相关性减小，单树分类能力增加，是不是效果更好？可能单树的分类能力不如增大m的时候？

- 验证模型效果的方法
  - 袋外错误率(out-of-bag error)；随机森林因为每棵树的生成都没有用到全部样本，所以每棵树可以用剩下的数据作为测试数据；

## 深度学习相关

- 优点：端到端，没有过多人为处理(少出错)
- 特点：表征学习，通过端到端的训练，发现更好的特征，而后面用于分类（或其他任务）的函数，往往只是普通的

## 参考文档
lgbm
https://github.com/jiangnanboy/learning_to_rank

SHAP值解释模型特征
https://blog.csdn.net/u011984148/article/details/105803720

无偏估计：不考虑样本的出现概率，认为概率相同；

随机森林
https://blog.csdn.net/yangyin007/article/details/82385967
还有效果图

